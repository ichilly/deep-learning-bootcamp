import tensorflow as tf

# X and Y data
x_train = [1, 2, 3]
y_train = [1, 2, 3]

W = tf.Variable(tf.random_normal([1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

# hypothesis = X * W + b
hypothesis = x_train * W + b

# cost/loss function
cost = tf.reduce_mean(tf.square(hypothesis - y_train))

# GradientDescent
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)

# initialize session
sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(2001): # from 0 to 2000
    sess.run(train)
    if step % 20 == 0:   # print every 20 steps
        print(step, sess.run(cost), sess.run(W), sess.run(b))

# Output (step, cost, W, b)
#0 1.8434349 [1.7582694] [-0.30818468]
#20 0.054006886 [1.275097] [-0.4905733]
#40 0.034359727 [1.2195543] [-0.48626432]
#60 0.031072868 [1.2051784] [-0.46519664]
#80 0.028219664 [1.1951494] [-0.4435041]
#100 0.025629515 [1.1859412] [-0.4226772]
#120 0.023277136 [1.1771992] [-0.40281442]
#140 0.021140695 [1.1688712] [-0.3838838]
#160 0.019200312 [1.1609347] [-0.3658427]
#180 0.017438022 [1.1533715] [-0.34864938]
#200 0.015837474 [1.1461635] [-0.3322641]
#220 0.014383863 [1.1392944] [-0.31664893]
#240 0.013063665 [1.1327482] [-0.30176762]
#260 0.011864626 [1.1265094] [-0.2875857]
#280 0.010775622 [1.1205637] [-0.27407017]
#300 0.009786605 [1.1148977] [-0.2611898]
#320 0.008888349 [1.1094981] [-0.24891485]
#340 0.008072556 [1.1043521] [-0.23721682]
#360 0.007331626 [1.099448] [-0.2260686]
#380 0.0066587008 [1.0947744] [-0.21544425]
#400 0.0060475417 [1.0903203] [-0.20531923]
#420 0.005492477 [1.0860755] [-0.19566998]
#440 0.0049883593 [1.0820305] [-0.18647419]
#460 0.0045305104 [1.0781752] [-0.17771064]
#480 0.00411467 [1.0745012] [-0.16935888]
#500 0.0037370091 [1.0709999] [-0.16139957]
#520 0.003394012 [1.0676632] [-0.15381436]
#540 0.0030825 [1.0644833] [-0.1465857]
#560 0.002799578 [1.0614531] [-0.13969678]
#580 0.002542619 [1.0585647] [-0.13313155]
#600 0.0023092462 [1.0558125] [-0.12687482]
#620 0.002097299 [1.0531895] [-0.12091217]
#640 0.001904788 [1.0506896] [-0.11522975]
#660 0.0017299671 [1.0483075] [-0.10981432]
#680 0.001571185 [1.0460372] [-0.10465348]
#700 0.0014269743 [1.0438738] [-0.09973519]
#720 0.0012960046 [1.041812] [-0.09504803]
#740 0.0011770568 [1.0398469] [-0.09058128]
#760 0.0010690166 [1.0379741] [-0.08632422]
#780 0.00097089546 [1.0361894] [-0.08226725]
#800 0.0008817818 [1.0344887] [-0.07840098]
#820 0.0008008532 [1.0328679] [-0.07471642]
#840 0.0007273434 [1.0313232] [-0.07120503]
#860 0.00066058506 [1.0298511] [-0.06785858]
#880 0.00059995445 [1.0284482] [-0.06466949]
#900 0.00054488826 [1.0271113] [-0.06163024]
#920 0.00049487635 [1.0258372] [-0.0587339]
#940 0.0004494557 [1.0246229] [-0.05597361]
#960 0.00040820372 [1.0234658] [-0.05334307]
#980 0.00037073749 [1.0223631] [-0.05083618]
#1000 0.00033671022 [1.0213121] [-0.04844717]
#1020 0.0003058042 [1.0203103] [-0.0461703]
#1040 0.00027773276 [1.0193557] [-0.04400031]
#1060 0.00025224447 [1.0184461] [-0.04193242]
#1080 0.0002290905 [1.0175792] [-0.03996171]
#1100 0.00020806408 [1.0167531] [-0.03808365]
#1120 0.00018896589 [1.0159657] [-0.03629387]
#1140 0.00017162283 [1.0152154] [-0.03458815]
#1160 0.00015587134 [1.0145003] [-0.03296264]
#1180 0.00014156428 [1.0138189] [-0.0314135]
#1200 0.0001285702 [1.0131694] [-0.02993718]
#1220 0.00011677109 [1.0125505] [-0.02853023]
#1240 0.00010605227 [1.0119606] [-0.02718942]
#1260 9.6318465e-05 [1.0113986] [-0.0259116]
#1280 8.747688e-05 [1.0108628] [-0.02469386]
#1300 7.944842e-05 [1.0103525] [-0.02353337]
#1320 7.215758e-05 [1.0098659] [-0.02242747]
#1340 6.553325e-05 [1.0094022] [-0.02137343]
#1360 5.9518934e-05 [1.0089602] [-0.0203689]
#1380 5.4056043e-05 [1.0085392] [-0.0194116]
#1400 4.9094233e-05 [1.008138] [-0.01849932]
#1420 4.4588916e-05 [1.0077554] [-0.01762993]
#1440 4.0496056e-05 [1.007391] [-0.01680142]
#1460 3.6779285e-05 [1.0070437] [-0.01601182]
#1480 3.34033e-05 [1.0067127] [-0.01525939]
#1500 3.0337274e-05 [1.0063971] [-0.01454228]
#1520 2.7553406e-05 [1.0060965] [-0.01385882]
#1540 2.502447e-05 [1.00581] [-0.0132075]
#1560 2.2727618e-05 [1.005537] [-0.01258682]
#1580 2.0641926e-05 [1.0052768] [-0.01199532]
#1600 1.8747443e-05 [1.0050288] [-0.01143165]
#1620 1.7026281e-05 [1.0047925] [-0.01089442]
#1640 1.5464086e-05 [1.0045673] [-0.01038244]
#1660 1.4044809e-05 [1.0043527] [-0.00989452]
#1680 1.2755988e-05 [1.0041481] [-0.00942954]
#1700 1.15849025e-05 [1.0039532] [-0.00898643]
#1720 1.0521664e-05 [1.0037674] [-0.00856411]
#1740 9.556159e-06 [1.0035902] [-0.00816161]
#1760 8.678892e-06 [1.0034217] [-0.00777803]
#1780 7.88257e-06 [1.0032609] [-0.00741253]
#1800 7.159009e-06 [1.0031077] [-0.00706423]
#1820 6.5021445e-06 [1.0029616] [-0.00673229]
#1840 5.905727e-06 [1.0028225] [-0.00641595]
#1860 5.363427e-06 [1.0026898] [-0.00611446]
#1880 4.871271e-06 [1.0025635] [-0.00582717]
#1900 4.4241824e-06 [1.002443] [-0.00555338]
#1920 4.018101e-06 [1.0023283] [-0.00529242]
#1940 3.649612e-06 [1.0022188] [-0.00504374]
#1960 3.3145607e-06 [1.0021145] [-0.00480674]
#1980 3.0101971e-06 [1.0020151] [-0.00458086]
#2000 2.734001e-06 [1.0019206] [-0.00436564]
